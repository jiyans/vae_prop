# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.15.0
#   kernelspec:
#     display_name: python3
#     language: python
#     name: python3
# ---

# %% [markdown]
# # vae_prop
#
# > Trying to use VAE's as a replacement for propensity scores.

# %%
# | default_exp core

# %%
# | hide
from nbdev.showdoc import *


# %%
# | export
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# Generate synthetic data
np.random.seed(42)
n_samples = 1000

# %%

# Let's create variables with causal relationships
athletic_ability = np.random.normal(0, 1, n_samples)
academic_performance = np.random.normal(0, 1, n_samples)
# Scholarship is influenced by both (collider)
scholarship = (
    0.7 * athletic_ability
    + 0.7 * academic_performance
    + np.random.normal(0, 0.1, n_samples)
)

# Stack them together
X = np.column_stack([athletic_ability, academic_performance, scholarship])

# Convert to PyTorch tensors
X_tensor = torch.FloatTensor(X)


# Simple autoencoder
class Autoencoder(nn.Module):
    def __init__(self, input_dim=3, latent_dim=2):
        super().__init__()
        self.encoder = nn.Sequential(nn.Linear(input_dim, latent_dim), nn.ReLU())
        self.decoder = nn.Sequential(nn.Linear(latent_dim, input_dim))

    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return reconstructed, latent


# Train the autoencoder
model = Autoencoder()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training loop
n_epochs = 100
for epoch in range(n_epochs):
    optimizer.zero_grad()
    reconstructed, latent = model(X_tensor)
    loss = criterion(reconstructed, X_tensor)
    loss.backward()
    optimizer.step()

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

# Get latent representations
with torch.no_grad():
    _, latent_repr = model(X_tensor)
    latent_repr = latent_repr.numpy()

# Analyze relationships in latent space
print("\nCorrelations in original space:")
print("Athletic-Academic:", np.corrcoef(athletic_ability, academic_performance)[0, 1])
print("Athletic-Scholarship:", np.corrcoef(athletic_ability, scholarship)[0, 1])
print("Academic-Scholarship:", np.corrcoef(academic_performance, scholarship)[0, 1])

print("\nCorrelations in latent space:")
print("Latent1-Latent2:", np.corrcoef(latent_repr[:, 0], latent_repr[:, 1])[0, 1])

# Let's see what each latent dimension correlates with
print("\nCorrelations between latent dimensions and original variables:")
print("Latent1-Athletic:", np.corrcoef(latent_repr[:, 0], athletic_ability)[0, 1])
print("Latent1-Academic:", np.corrcoef(latent_repr[:, 0], academic_performance)[0, 1])
print("Latent1-Scholarship:", np.corrcoef(latent_repr[:, 0], scholarship)[0, 1])
print("Latent2-Athletic:", np.corrcoef(latent_repr[:, 1], athletic_ability)[0, 1])
print("Latent2-Academic:", np.corrcoef(latent_repr[:, 1], academic_performance)[0, 1])
print("Latent2-Scholarship:", np.corrcoef(latent_repr[:, 1], scholarship)[0, 1])


# %%
# | hide
import nbdev

nbdev.nbdev_export()
